I Tried to Build an AI Trading System — and Ended Up Learning How Real Quant Research Actually Works

This is not a story about beating the market.
It’s a story about why most AI trading projects fail, and what happens when you try to do one correctly.

1. The Motivation (and the Naive Beginning)

Like many engineering students interested in machine learning and finance, I started with a simple question:

Can I build an AI that trades stocks?

At first, I thought the answer would involve:

predicting prices

training fancy neural networks

optimizing accuracy

But after looking at dozens of “AI trading” projects online, I noticed a pattern:

Most of them were fundamentally broken.

They:

leaked future data

optimized meaningless rewards

overtraded aggressively

looked profitable in backtests but would collapse in reality

I didn’t want to build something that looked impressive.
I wanted to build something that was correct — even if it wasn’t profitable.

That decision changed everything.

2. The First Mental Shift: Prediction Is Not Trading

One realization changed the direction of the entire project:

Trading is not prediction. Trading is decision-making under uncertainty.

In real trading:

You decide when to enter

You decide when to exit

You manage risk and capital

Accuracy doesn’t matter — PnL does

That naturally led me to Reinforcement Learning (RL).

RL doesn’t predict prices.
It learns policies — sequences of decisions — based on consequences.

That felt much closer to how real traders operate.

3. The System I Built (At a High Level)

The core system looks like this:

Market Data → CNN + LSTM Encoder → PPO Agent → Buy / Sell / Hold


CNN learns short-term price patterns

LSTM captures temporal context

PPO learns a trading policy

A custom trading environment simulates balance, positions, and transaction costs

This wasn’t a “model”.
It was a decision-making system.

4. Reward Design: Where Everything Went Wrong

The hardest part of the project wasn’t neural networks.
It was reward design.

Initially, I tried things like:

rewarding unrealized profits

penalizing too many trades

giving small rewards for holding winners

The result was disastrous.

The agent:

traded constantly

accumulated positive rewards

ended with zero real profit

This is called reward hacking.

The agent wasn’t learning to trade.
It was learning to exploit my reward function.

5. The Breakthrough: Portfolio-Value Reward

Eventually, I realized something fundamental:

If the reward is not money, the agent will not care about money.

So I changed the reward to a single, brutally simple rule:

reward = change in total portfolio value


Where:

portfolio value = balance + (position × current price)


This one change:

eliminated reward hacking

penalized overtrading naturally (via transaction costs)

forced the agent to care about actual money

This was the moment the project stopped being a toy.

6. What “Good” RL Training Actually Looks Like

Another surprise: good RL training looks boring.

Healthy training looked like:

stable KL divergence

low value loss

explained variance close to 1

no wild oscillations

Unhealthy training looked like:

huge early profits

extreme volatility

constant trading

When my PPO training converged quietly, I realized something important:

Stability matters more than excitement.

7. Walk-Forward Testing (Where Most Projects Cheat)

This is where most ML trading projects quietly fail.

I enforced a strict walk-forward setup:

Training data: 2018–2021

Test data: 2022–2024

Rules:

no retraining on test data

no hyperparameter tuning

same frozen policy

This mimics real deployment:

Train on the past. Deploy into the future.

Anything else is data leakage.

8. Market Regimes — Without Feeding Indicators to the Model

To understand behavior, I classified the test period into:

trending markets

sideways markets

volatile markets

Important detail:

These regime labels were used only for analysis, not as model inputs.

They were computed after trading, using:

log returns

rolling volatility

The agent never “knew” the regime.
I used them only to interpret behavior.

9. The Results (and Why They Mattered)

On completely unseen data, the agent behaved like this:

One surprising outcome was that the agent did not primarily trade during trending regimes. Instead, it executed a larger number of trades during sideways market conditions, generating modest positive returns, while experiencing losses during some trending periods. This suggests that the learned policy behaves more like a short-horizon or mean-reversion strategy rather than a classical trend follower. Importantly, this behavior emerged without any explicit regime labels or indicators provided to the model, highlighting how reinforcement learning policies can converge to non-obvious strategies under realistic constraints.

This shocked me.

The agent wasn’t trying to trade everywhere.
It was being selective.

That’s exactly how real traders behave.

10. Debugging Hell (and Why It Was Worth It)

This project was not smooth.

I ran into:

NaNs from log(0) in price data

off-by-one errors in regime alignment

inconsistent evaluation outputs

confusion around determinism in RL

Each bug forced me to learn something real:

numerical stability matters

evaluation must be deterministic

encoders must be in .eval() mode

rerunning experiments casually breaks scientific validity

These weren’t beginner mistakes.
They were research-level lessons.

11. Why Results Changed When I Re-Ran Evaluation

At one point, I re-ran evaluation and saw different results — even with deterministic actions.

That taught me a crucial rule of research:

Deterministic actions do not guarantee deterministic experiments.

Encoder modes, seeds, and evaluation context matter.

I learned to:

lock seeds

freeze models

run evaluation once

save results and move on

That discipline matters more than squeezing out better numbers.

12. What I Actually Built (Honestly)

I did not build:

a stock price predictor

a get-rich-quick bot

an AI that beats the market

I built:

a correct RL trading system

with realistic constraints

evaluated honestly

that behaves sensibly under different market conditions

That’s rare — especially at a student level.

13. Why This Matters (Even Without Big Profits)

In finance:

one profitable backtest means nothing

correctness means everything

This project demonstrates:

understanding of RL beyond tutorials

awareness of data leakage

respect for market realism

ability to debug subtle system-level issues

Those are the skills real quants care about.

14. This Is Only Phase 1

What I’ve described here is Phase 1.

Next phases will include:

explicit risk-aware rewards (drawdown control)

multi-asset training

execution delays and slippage

statistical ablation studies

I’ll document each phase honestly — including failures.

Final Thought

The biggest lesson so far:

If a trading project looks impressive too early, it’s probably wrong.

This one looked boring for a long time.

That’s how I knew it was real.

This post documents Phase 1 of an ongoing project.

I’ll continue this series as I move into risk-aware objectives, multi-asset learning, and execution modeling.

If you’re interested, stay tuned — the hard parts are just beginning.



Part 2 — Teaching the Agent to Fear Losses: Risk-Aware Reinforcement Learning

This post continues Phase 1 of my reinforcement-learning trading project. If you haven’t read it, Part 1 covers the motivation, architecture, reward hacking issues, and walk-forward regime evaluation.

15. A Hard Realization After Phase 1

Phase 1 left me with an uncomfortable but honest result.

The agent:

sometimes made money

sometimes lost money

behaved very differently across market regimes

Most importantly, it did not care how bad the losses were.

It only cared about step-to-step profit.

That raised a serious question:

What if the agent makes money overall, but experiences large drawdowns along the way?

In real trading, that’s unacceptable.

No professional trader, fund, or desk optimizes raw profit alone.
They optimize profit under risk constraints.

That’s when Phase 2 began.

16. Why Profit Alone Is Not Enough

In Phase 1, the reward was:

reward = change in portfolio value


This is correct — but incomplete.

Because it teaches the agent:

“making money is good”

but not:

“losing a lot is very bad”

In real markets:

two strategies can have the same profit

but vastly different drawdowns

and only one would ever be deployed

So I needed to teach the agent risk awareness.

17. Drawdown — Explained Without Math

Imagine this equity curve:

10000 → 10200 → 10100 → 10300 → 9800


Even if the final result recovers later, that drop from 10300 to 9800 is painful.

That drop is called drawdown.

Drawdown measures:

How far you fall from your best point so far.

Quants care about drawdown because:

it reflects psychological pressure

capital constraints

real-world survivability

18. The Risk-Aware Reward (The Core Change)

In Phase 2, I changed the reward to:

reward = Δ portfolio value − λ × drawdown


This does one crucial thing:

It tells the agent that large losses are worse than missed profits.

The agent is no longer just a money-seeker.
It becomes loss-averse.

This single modification completely changed behavior.

19. Training Again — From Scratch (No Shortcuts)

Because the objective changed, I retrained everything from scratch:

Same PPO algorithm

Same CNN + LSTM encoder

Same walk-forward split:

Train: 2018–2021

Test: 2022–2024

No retraining on test data

No tuning on results

Changing the reward means changing the problem.
So the evaluation protocol had to remain identical.

This is non-negotiable in research.

20. Phase 2 Walk-Forward Results

On unseen test data, the behavior shifted dramatically.

The agent now showed:

Sideways markets

highest profitability

many small trades

very smooth equity curve

Volatile markets

reduced exposure

smaller position sizes

controlled losses

Trending markets

early exits

missed long continuations

overall negative PnL

At first glance, this looks disappointing.

But it’s actually exactly what we asked the agent to do.

21. Understanding the Trade-Off (This Is the Key Insight)

Trends are not smooth lines.

They contain:

pullbacks

temporary losses

noise before continuation

By penalizing drawdown, I told the agent:

“If holding through pain is risky, don’t do it.”

So the agent:

exited trends early

avoided deep pullbacks

sacrificed upside for safety

This is not a bug.

This is the cost of risk aversion.

22. Why This Result Is More Valuable Than Profit

Phase 2 taught me something far more important than “how to make money”.

It showed that:

reward design directly shapes behavior

different objectives produce fundamentally different strategies

there is no single “best” trading policy

Instead, there are trade-offs.

This is exactly how real quant research works:

define objective

observe behavior

accept limitations

iterate carefully

23. Phase 1 vs Phase 2 — Conceptually

Phase 1 agent:

opportunistic

sometimes reckless

profit-seeking

sensitive to regimes

Phase 2 agent:

conservative

drawdown-aware

survives volatility better

avoids large losses

gives up on trends

Neither is “better” in isolation.

They solve different problems.

24. What Phase 2 Proved

By the end of Phase 2, I could confidently say:

the system generalizes to unseen data

behavior changes predictably with objective changes

risk can be explicitly encoded into RL rewards

regime-dependent performance is unavoidable and explainable

This is not a toy project anymore.

This is experimental reinforcement-learning research.

25. Why I Did Not “Fix” the Trend Losses Yet

It’s tempting to immediately tweak things:

reduce drawdown penalty

add trend indicators

force longer holding

I deliberately did none of that.

Because doing so would mix:

objective design

feature engineering

regime awareness

And that would destroy interpretability.

Before fixing behavior, you must understand why it exists.

Phase 2 gave me that understanding.

26. Where This Naturally Leads Next

Phase 2 answered this question:

What happens when an RL trader is explicitly punished for risk?

The next question is harder:

Can we balance risk control and opportunity capture at the same time?

That’s where:

multi-asset learning

shared policies

richer state representations

come into play.

But that is Phase 3.

Closing Thoughts Before Phase 3

Up to this point, the project has not been about:

beating benchmarks

maximizing profit

showing flashy curves

It has been about:

correctness

discipline

honesty

understanding cause and effect

Most trading projects skip these steps.

I didn’t.

This concludes Phase 2 of the project.

In the next part, I’ll explore whether training across multiple assets can stabilize behavior and recover lost opportunities without sacrificing risk control.

Phase 3 is where things get truly interesting.


Phase 3 — When Everything Broke (and Finally Made Sense)

Up until Phase 2, my reinforcement learning trading system looked clean.

I had:

A CNN + LSTM encoder extracting market features

A PPO agent trading a single asset

A risk-aware reward penalizing drawdowns

Walk-forward testing that actually made sense

But Phase 3 changed the nature of the problem completely.

Instead of training one agent per stock, I forced myself to ask a harder question:

Can a single RL policy generalize across multiple assets without blowing up?

This is where things stopped being “student-friendly” and started feeling like real research.

Phase 3.1 — Moving from Single-Asset to Multi-Asset RL

The idea was simple in theory:

One shared PPO policy

One shared CNN-LSTM encoder

Multiple stocks (AAPL, MSFT, NVDA, TSLA)

Random asset per episode during training

In practice, this required building a Multi-Asset Trading Environment that:

Randomly selects an asset at reset

Shares the same action space

Preserves realistic trading mechanics

This was the first time I realized:

Reinforcement learning doesn’t fail loudly — it fails silently if your environment is even slightly wrong.

Phase 3.2 — Gym, Gymnasium, and Why My Code Kept Crashing

The moment I plugged my custom environment into Stable-Baselines3, everything exploded.

Errors like:

env.reset() returning the wrong number of values

env.step() returning 4 values when 5 were expected

Missing seed() methods

Silent API mismatches between Gym and Gymnasium

At first, it felt like “library hell”.

But this phase taught me something critical:

Modern RL libraries assume Gymnasium semantics, even if you’re still using Gym.

I had to:

Rewrite reset() to return (obs, info)

Rewrite step() to return (obs, reward, terminated, truncated, info)

Add explicit seed() handling

Ensure deterministic resets for reproducibility

This wasn’t busywork — it was enforcing correct RL contracts.

Phase 3.3 — The Terminal State Trap (Off-By-One Hell)

Even after fixing APIs, my environment kept crashing at the end of episodes.

Index errors like:

index out of bounds for dimension 0


The root cause?

I was accessing prices before checking termination

Terminal states were still trying to build new observations

My environment assumed “one more step” always existed

This led to a fundamental realization:

In RL, terminal state handling is harder than action logic.

The fix required:

Guarding termination before any data access

Storing the last valid observation

Returning dummy observations at termination

Never computing rewards using invalid indices

This is the kind of bug that only appears when evaluation is actually correct.

Phase 3.4 — The Zero-Trade Mystery

Once the environment finally ran end-to-end, I saw something strange:

The agent trained successfully

Evaluation completed

But the number of trades was zero across all assets

At first, I assumed something was broken.

It wasn’t.

The agent had learned a loophole.

Phase 3.5 — Policy Collapse Isn’t a Bug

My action space was:

0 → HOLD

1 → BUY

2 → SELL

The PPO policy learned:

“If I always SELL while holding no position, I minimize risk and avoid penalties.”

This was a degenerate optimal policy under my reward.

It wasn’t stupidity — it was mathematical correctness.

This is where I learned a brutal RL lesson:

If your action space allows invalid actions, PPO will exploit them.

Phase 3.6 — Fixing the Action Space (The Right Way)

Instead of forcing trades or hacking rewards, I fixed the environment properly:

SELL was masked when no position existed

BUY was masked when already holding

HOLD received a tiny inactivity penalty

Transaction costs applied only on execution

This immediately changed agent behavior.

The policy now had to:

BUY before it could SELL

Accept exposure risk to earn reward

Trade like a real market participant

Phase 3.7 — Why “Number of Trades” Was Lying

Even after this fix, trade counts still looked wrong.

That turned out to be an evaluation mistake, not an RL one.

I was counting round trips instead of executed actions.

In reality:

A BUY is a trade

A SELL is a trade

A forced liquidation is also a trade

Once I switched to execution-based trade counting, the results finally aligned with reality.

Phase 3.8 — The Final Multi-Asset Results

After all fixes, retraining, and clean evaluation, the shared PPO policy produced:

Different behavior per asset

Non-zero executed trades

Modest but consistent PnL

No asset blow-ups

Stable risk-aware behavior

Example outcomes:

AAPL and NVDA → moderate profits

MSFT → near-flat

TSLA → fewer but higher-impact trades

This confirmed something important:

A single PPO policy can generalize across assets — but it becomes conservative by design.

Phase 3 — What I Learned (The Real Takeaways)

This phase taught me more than any tutorial ever could:

Environment design matters more than models

PPO will exploit any loophole you leave open

Risk-aware objectives suppress aggressive trading

Multi-asset generalization forces conservatism

Evaluation bugs are more dangerous than training bugs

Honest results are more valuable than flashy backtests

Most importantly:

Reinforcement learning trading isn’t about “making money in backtests” — it’s about understanding why a policy behaves the way it does.

Where This Leads Next

Phase 3 answered whether a shared policy can generalize.

It did.

But it also raised a deeper question:

How does risk appetite shape trading behavior?

That’s what comes next.